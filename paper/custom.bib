
@inproceedings{buszydlik_red_2023,
	address = {Bali, Indonesia},
	title = {Red {Teaming} for {Large} {Language} {Models} {At} {Scale}: {Tackling} {Hallucinations} on {Mathematics} {Tasks}},
	shorttitle = {Red {Teaming} for {Large} {Language} {Models} {At} {Scale}},
	url = {https://aclanthology.org/2023.artofsafety-1.1/},
	doi = {10.18653/v1/2023.artofsafety-1.1},
	urldate = {2025-06-21},
	booktitle = {Proceedings of the {ART} of {Safety}: {Workshop} on {Adversarial} testing and {Red}-{Teaming} for generative {AI}},
	publisher = {Association for Computational Linguistics},
	author = {Buszydlik, Aleksander and Dobiczek, Karol and Okoń, Michał Teodor and Skublicki, Konrad and Lippmann, Philip and Yang, Jie},
	editor = {Parrish, Alicia},
	month = nov,
	year = {2023},
	pages = {1--10},
}

@article{buszydlik_grounding_2024,
	title = {Grounding and {Validation} of {Algorithmic} {Recourse} in {Real}-{World} {Contexts}: {A} {Systematized} {Literature} {Review}},
	shorttitle = {Grounding and {Validation} of {Algorithmic} {Recourse} in {Real}-{World} {Contexts}},
	url = {https://openreview.net/forum?id=oEmyoy5H5P},
	abstract = {The aim of algorithmic recourse (AR) is generally understood to be the provision of "actionable" recommendations to individuals affected by algorithmic decision-making systems, in an attempt to offer the capacity for taking actions that may lead to more desirable outcomes in the future. Over the past few years, AR literature has largely focused on theoretical frameworks to generate "actionable" counterfactual explanations that further satisfy various desiderata, such as diversity or robustness. We believe that algorithmic recourse, by its nature, should be seen as a practical problem: real-world socio-technical decision-making systems are complex dynamic entities involving various actors (end users, domain experts, civil servants, system owners, etc.) engaged in social and technical processes. Thus, research needs to account for the specificities of systems where it would be applied. To evaluate how authors envision AR "in the wild", we carry out a systematized review of 127 publications pertaining to the problem and identify the real-world considerations that motivate them. Among others, we look at the ways to make recourse (individually) actionable, the involved stakeholders, the perceived challenges, and the availability of practitioner-friendly open-source codebases. We find that there is a strong disconnect between the existing research and the practical requirements for AR. Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored. As an attempt to bridge this gap, we provide other authors with five recommendations to make future solutions easier to adapt to their potential real-world applications.},
	language = {en},
	urldate = {2025-06-21},
	author = {Buszydlik, Aleksander and Altmeyer, Patrick and Liem, Cynthia C. S. and Dobbe, Roel},
	month = may,
	year = {2024},
}

@inproceedings{liu_multi-aspect_2024,
	address = {Bangkok, Thailand},
	title = {Multi-{Aspect} {Controllable} {Text} {Generation} with {Disentangled} {Counterfactual} {Augmentation}},
	url = {https://aclanthology.org/2024.acl-long.500},
	doi = {10.18653/v1/2024.acl-long.500},
	language = {en},
	urldate = {2025-06-21},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Yi and Liu, Xiangyu and Zhu, Xiangrong and Hu, Wei},
	year = {2024},
	pages = {9231--9253},
}

@incollection{longo_courge_2023,
	address = {Cham},
	title = {{CouRGe}: {Counterfactual} {Reviews} {Generator} for {Sentiment} {Analysis}},
	volume = {1662},
	isbn = {9783031264375 9783031264382},
	shorttitle = {{CouRGe}},
	url = {https://link.springer.com/10.1007/978-3-031-26438-2_24},
	abstract = {Abstract 
            Past literature in Natural Language Processing (NLP) has demonstrated that counterfactual data points are useful, for example, for increasing model generalisation, enhancing model interpretability, and as a data augmentation approach. However, obtaining counterfactual examples often requires human annotation effort, which is an expensive and highly skilled process. For these reasons, solutions that resort to transformer-based language models have been recently proposed to generate counterfactuals automatically, but such solutions show limitations. 
            In this paper, we present CouRGe, a language model that, given a movie review (i.e. a seed review) and its sentiment label, generates a counterfactual review that is close (similar) to the seed review but of the opposite sentiment. CouRGe is trained by supervised fine-tuning of GPT-2 on a task-specific dataset of paired movie reviews, and its generation is prompt-based. The model does not require any modification to the network’s architecture or the design of a specific new task for fine-tuning. 
            Experiments show that CouRGe’s generation is effective at flipping the seed sentiment and produces counterfactuals reasonably close to the seed review. This proves once again the great flexibility of language models towards downstream tasks as hard as counterfactual reasoning and opens up the use of CouRGe’s generated counterfactuals for the applications mentioned above.},
	language = {en},
	urldate = {2025-06-21},
	booktitle = {Artificial {Intelligence} and {Cognitive} {Science}},
	publisher = {Springer Nature Switzerland},
	author = {Carraro, Diego and Brown, Kenneth N.},
	editor = {Longo, Luca and O’Reilly, Ruairi},
	year = {2023},
	doi = {10.1007/978-3-031-26438-2_24},
	pages = {305--317},
}

@misc{zheng_f-fidelity_2024,
	title = {F-{Fidelity}: {A} {Robust} {Framework} for {Faithfulness} {Evaluation} of {Explainable} {AI}},
	shorttitle = {F-{Fidelity}},
	url = {http://arxiv.org/abs/2410.02970},
	doi = {10.48550/arXiv.2410.02970},
	abstract = {Recent research has developed a number of eXplainable AI (XAI) techniques. Although extracting meaningful insights from deep learning models, how to properly evaluate these XAI methods remains an open problem. The most widely used approach is to perturb or even remove what the XAI method considers to be the most important features in an input and observe the changes in the output prediction. This approach although efficient suffers the Out-of-Distribution (OOD) problem as the perturbed samples may no longer follow the original data distribution. A recent method RemOve And Retrain (ROAR) solves the OOD issue by retraining the model with perturbed samples guided by explanations. However, the training may not always converge given the distribution difference. Furthermore, using the model retrained based on XAI methods to evaluate these explainers may cause information leakage and thus lead to unfair comparisons. We propose Fine-tuned Fidelity F-Fidelity, a robust evaluation framework for XAI, which utilizes i) an explanation-agnostic fine-tuning strategy, thus mitigating the information leakage issue and ii) a random masking operation that ensures that the removal step does not generate an OOD input. We designed controlled experiments with state-of-the-art (SOTA) explainers and their degraded version to verify the correctness of our framework. We conducted experiments on multiple data structures, such as images, time series, and natural language. The results demonstrate that F-Fidelity significantly improves upon prior evaluation metrics in recovering the ground-truth ranking of the explainers. Furthermore, we show both theoretically and empirically that, given a faithful explainer, F-Fidelity metric can be used to compute the sparsity of influential input components, i.e., to extract the true explanation size.},
	urldate = {2025-02-16},
	publisher = {arXiv},
	author = {Zheng, Xu and Shirani, Farhad and Chen, Zhuomin and Lin, Chaohao and Cheng, Wei and Guo, Wenbo and Luo, Dongsheng},
	month = oct,
	year = {2024},
	note = {arXiv:2410.02970},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{agarwal_faithfulness_2024,
	title = {Faithfulness vs. {Plausibility}: {On} the ({Un}){Reliability} of {Explanations} from {Large} {Language} {Models}},
	shorttitle = {Faithfulness vs. {Plausibility}},
	url = {http://arxiv.org/abs/2402.04614},
	doi = {10.48550/arXiv.2402.04614},
	abstract = {Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Agarwal, Chirag and Tanneru, Sree Harsha and Lakkaraju, Himabindu},
	month = mar,
	year = {2024},
	note = {arXiv:2402.04614},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{nguyen_ceval_2024,
	address = {Tokyo, Japan},
	title = {{CEval}: {A} {Benchmark} for {Evaluating} {Counterfactual} {Text} {Generation}},
	shorttitle = {{CEval}},
	url = {https://aclanthology.org/2024.inlg-main.6/},
	abstract = {Counterfactual text generation aims to minimally change a text, such that it is classified differently. Assessing progress in method development for counterfactual text generation is hindered by a non-uniform usage of data sets and metrics in related work. We propose CEval, a benchmark for comparing counterfactual text generation methods. CEval unifies counterfactual and text quality metrics, includes common counterfactual datasets with human annotations, standard baselines (MICE, GDBA, CREST) and the open-source language model LLAMA-2. Our experiments found no perfect method for generating counterfactual text. Methods that excel at counterfactual metrics often produce lower-quality text while LLMs with simple prompts generate high-quality text but struggle with counterfactual criteria. By making CEval available as an open-source Python library, we encourage the community to contribute additional methods and maintain consistent evaluation in future work.},
	urldate = {2025-02-09},
	booktitle = {Proceedings of the 17th {International} {Natural} {Language} {Generation} {Conference}},
	publisher = {Association for Computational Linguistics},
	author = {Nguyen, Van Bach and Seifert, Christin and Schlötterer, Jörg},
	editor = {Mahamood, Saad and Minh, Nguyen Le and Ippolito, Daphne},
	month = sep,
	year = {2024},
	pages = {55--69},
}

@misc{lu_does_2024,
	title = {Does {Faithfulness} {Conflict} with {Plausibility}? {An} {Empirical} {Study} in {Explainable} {AI} across {NLP} {Tasks}},
	shorttitle = {Does {Faithfulness} {Conflict} with {Plausibility}?},
	url = {http://arxiv.org/abs/2404.00140},
	doi = {10.48550/arXiv.2404.00140},
	abstract = {Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) {\textbackslash}textit\{faithfulness\}, where explanations accurately reflect the model's inference process. 2) {\textbackslash}textit\{plausibility\}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: sentiment analysis, intent detection, and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high levels of accuracy and user accessibility in their explanations.},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Lu, Xiaolei and Ma, Jianghong},
	month = mar,
	year = {2024},
	note = {arXiv:2404.00140},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{atanasova_faithfulness_2023,
	title = {Faithfulness {Tests} for {Natural} {Language} {Explanations}},
	url = {http://arxiv.org/abs/2305.18029},
	doi = {10.48550/arXiv.2305.18029},
	abstract = {Explanations of neural models aim to reveal a model's decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model's inner workings. This work explores the challenging question of evaluating the faithfulness of natural language explanations (NLEs). To this end, we present two tests. First, we propose a counterfactual input editor for inserting reasons that lead to counterfactual predictions but are not reflected by the NLEs. Second, we reconstruct inputs from the reasons stated in the generated NLEs and check how often they lead to the same predictions. Our tests can evaluate emerging NLE models, proving a fundamental tool in the development of faithful NLEs.},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Atanasova, Pepa and Camburu, Oana-Maria and Lioma, Christina and Lukasiewicz, Thomas and Simonsen, Jakob Grue and Augenstein, Isabelle},
	month = jun,
	year = {2023},
	note = {arXiv:2305.18029},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@book{murphy_probabilistic_2022,
	title = {Probabilistic {Machine} {Learning}: {An} introduction},
	url = {probml.ai},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	year = {2022},
}

@misc{zhang_opt_2022,
	title = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
	shorttitle = {{OPT}},
	url = {http://arxiv.org/abs/2205.01068},
	doi = {10.48550/arXiv.2205.01068},
	abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
	urldate = {2024-08-23},
	publisher = {arXiv},
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
	month = jun,
	year = {2022},
	note = {arXiv:2205.01068 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{chen_models_2023,
	title = {Do {Models} {Explain} {Themselves}? {Counterfactual} {Simulatability} of {Natural} {Language} {Explanations}},
	shorttitle = {Do {Models} {Explain} {Themselves}?},
	url = {https://openreview.net/forum?id=VvAiCXwPvD},
	abstract = {Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate \${\textbackslash}textbf\{counterfactual simulatability\}\$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers ''\${\textbackslash}textit\{yes\}\$'' to the input question ''\${\textbackslash}textit\{Can eagles fly?\}\$'' with the explanation ''\${\textbackslash}textit\{all birds can fly\}\$'', then humans would infer from the explanation that it would also answer ''\${\textbackslash}textit\{yes\}\$'' to the counterfactual input ''\${\textbackslash}textit\{Can penguins fly?\}\$''. If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.},
	language = {en},
	urldate = {2024-08-23},
	author = {Chen, Yanda and Zhong, Ruiqi and Ri, Narutatsu and Zhao, Chen and He, He and Steinhardt, Jacob and Yu, Zhou and McKeown, Kathleen},
	month = oct,
	year = {2023},
}

@misc{meister_language_2021,
	title = {Language {Model} {Evaluation} {Beyond} {Perplexity}},
	url = {http://arxiv.org/abs/2106.00085},
	doi = {10.48550/arXiv.2106.00085},
	abstract = {We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework--paired with significance tests--for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type--token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.},
	urldate = {2024-08-23},
	publisher = {arXiv},
	author = {Meister, Clara and Cotterell, Ryan},
	month = aug,
	year = {2021},
	note = {arXiv:2106.00085 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {https://www.semanticscholar.org/paper/PyTorch%3A-An-Imperative-Style%2C-High-Performance-Deep-Paszke-Gross/3c8a456509e6c0805354bd40a35e3f2dbf8069b1},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2024-08-23},
	journal = {ArXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, N. and Antiga, L. and Desmaison, Alban and Köpf, Andreas and Yang, E. and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
}

@misc{delft_high_performance_computing_centre_dhpc_delftblue_2024,
	title = {{DelftBlue} {Supercomputer} {Phase} 2},
	url = {https://www.tudelft.nl/dhpc/ark:/44463/DelftBluePhase2},
	author = {{Delft High Performance Computing Centre DHPC}},
	year = {2024},
}

@inproceedings{altmeyer_endogenous_2023,
	address = {Raleigh, NC, USA},
	title = {Endogenous {Macrodynamics} in {Algorithmic} {Recourse}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9781665462990},
	url = {https://ieeexplore.ieee.org/document/10136130/},
	doi = {10.1109/SaTML54575.2023.00036},
	urldate = {2024-08-22},
	booktitle = {2023 {IEEE} {Conference} on {Secure} and {Trustworthy} {Machine} {Learning} ({SaTML})},
	publisher = {IEEE},
	author = {Altmeyer, Patrick and Angela, Giovan and Buszydlik, Aleksander and Dobiczek, Karol and Van Deursen, Arie and Liem, Cynthia C. S.},
	month = feb,
	year = {2023},
	pages = {418--431},
}

@misc{joshi_towards_2019,
	title = {Towards {Realistic} {Individual} {Recourse} and {Actionable} {Explanations} in {Black}-{Box} {Decision} {Making} {Systems}},
	url = {http://arxiv.org/abs/1907.09615},
	doi = {10.48550/arXiv.1907.09615},
	abstract = {Machine learning based decision making systems are increasingly affecting humans. An individual can suffer an undesirable outcome under such decision making systems (e.g. denied credit) irrespective of whether the decision is fair or accurate. Individual recourse pertains to the problem of providing an actionable set of changes a person can undertake in order to improve their outcome. We propose a recourse algorithm that models the underlying data distribution or manifold. We then provide a mechanism to generate the smallest set of changes that will improve an individual's outcome. This mechanism can be easily used to provide recourse for any differentiable machine learning based decision making system. Further, the resulting algorithm is shown to be applicable to both supervised classification and causal decision making systems. Our work attempts to fill gaps in existing fairness literature that have primarily focused on discovering and/or algorithmically enforcing fairness constraints on decision making systems. This work also provides an alternative approach to generating counterfactual explanations.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Joshi, Shalmali and Koyejo, Oluwasanmi and Vijitbenjaronk, Warut and Kim, Been and Ghosh, Joydeep},
	month = jul,
	year = {2019},
	note = {arXiv:1907.09615 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{mothilal_explaining_2020,
	title = {Explaining {Machine} {Learning} {Classifiers} through {Diverse} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/1905.07697},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
	urldate = {2024-08-22},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	note = {arXiv:1905.07697 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {607--617},
}

@misc{antoran_getting_2021,
	title = {Getting a {CLUE}: {A} {Method} for {Explaining} {Uncertainty} {Estimates}},
	shorttitle = {Getting a {CLUE}},
	url = {http://arxiv.org/abs/2006.06848},
	doi = {10.48550/arXiv.2006.06848},
	abstract = {Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Antorán, Javier and Bhatt, Umang and Adel, Tameem and Weller, Adrian and Hernández-Lobato, José Miguel},
	month = mar,
	year = {2021},
	note = {arXiv:2006.06848 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{speith_review_2022,
	address = {Seoul Republic of Korea},
	title = {A {Review} of {Taxonomies} of {Explainable} {Artificial} {Intelligence} ({XAI}) {Methods}},
	isbn = {9781450393522},
	url = {https://dl.acm.org/doi/10.1145/3531146.3534639},
	doi = {10.1145/3531146.3534639},
	language = {en},
	urldate = {2024-08-22},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Speith, Timo},
	month = jun,
	year = {2022},
	pages = {2239--2250},
}

@inproceedings{sundararajan_axiomatic_2017,
	address = {Sydney, NSW, Australia},
	series = {{ICML}'17},
	title = {Axiomatic attribution for deep networks},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	urldate = {2024-08-22},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning} - {Volume} 70},
	publisher = {JMLR.org},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = aug,
	year = {2017},
	pages = {3319--3328},
}

@article{longo_explainable_2024,
	title = {Explainable {Artificial} {Intelligence} ({XAI}) 2.0: {A} manifesto of open challenges and interdisciplinary research directions},
	volume = {106},
	issn = {15662535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI}) 2.0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253524000794},
	doi = {10.1016/j.inffus.2024.102301},
	language = {en},
	urldate = {2024-08-22},
	journal = {Information Fusion},
	author = {Longo, Luca and Brcic, Mario and Cabitza, Federico and Choi, Jaesik and Confalonieri, Roberto and Ser, Javier Del and Guidotti, Riccardo and Hayashi, Yoichi and Herrera, Francisco and Holzinger, Andreas and Jiang, Richard and Khosravi, Hassan and Lecue, Freddy and Malgieri, Gianclaudio and Páez, Andrés and Samek, Wojciech and Schneider, Johannes and Speith, Timo and Stumpf, Simone},
	month = jun,
	year = {2024},
	pages = {102301},
}

@article{smales_classification_2023,
	title = {Classification of {RBA} monetary policy announcements using {ChatGPT}},
	volume = {58},
	issn = {15446123},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1544612323008863},
	doi = {10.1016/j.frl.2023.104514},
	language = {en},
	urldate = {2024-08-22},
	journal = {Finance Research Letters},
	author = {Smales, Lee A.},
	month = dec,
	year = {2023},
	pages = {104514},
}

@inproceedings{peskoff_gpt_2023,
	address = {Singapore},
	title = {{GPT} {Deciphering} {Fedspeak}: {Quantifying} {Dissent} {Among} {Hawks} and {Doves}},
	shorttitle = {{GPT} {Deciphering} {Fedspeak}},
	url = {https://aclanthology.org/2023.findings-emnlp.434},
	doi = {10.18653/v1/2023.findings-emnlp.434},
	abstract = {Markets and policymakers around the world hang on the consequential monetary policy decisions made by the Federal Open Market Committee (FOMC). Publicly available textual documentation of their meetings provides insight into members' attitudes about the economy. We use GPT-4 to quantify dissent among members on the topic of inflation. We find that transcripts and minutes reflect the diversity of member views about the macroeconomic outlook in a way that is lost or omitted from the public statements. In fact, diverging opinions that shed light upon the committee's “true” attitudes are almost entirely omitted from the final statements. Hence, we argue that forecasting FOMC sentiment based solely on statements will not sufficiently reflect dissent among the hawks and doves.},
	urldate = {2024-08-22},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Peskoff, Denis and Visokay, Adam and Schulhoff, Sander and Wachspress, Benjamin and Blinder, Alan and Stewart, Brandon},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {6529--6539},
}

@article{hansen_can_2023,
	title = {Can {ChatGPT} {Decipher} {Fedspeak}?},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4399406},
	doi = {10.2139/ssrn.4399406},
	language = {en},
	urldate = {2024-08-22},
	journal = {SSRN Electronic Journal},
	author = {Hansen, Anne Lundgaard and Kazinnik, Sophia},
	year = {2023},
}

@article{pfeifer_centralbankroberta_2023,
	title = {{CentralBankRoBERTa}: {A} fine-tuned large language model for central bank communications},
	volume = {9},
	issn = {24059188},
	shorttitle = {{CentralBankRoBERTa}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405918823000302},
	doi = {10.1016/j.jfds.2023.100114},
	language = {en},
	urldate = {2024-08-22},
	journal = {The Journal of Finance and Data Science},
	author = {Pfeifer, Moritz and Marohl, Vincent P.},
	month = nov,
	year = {2023},
	pages = {100114},
}

@misc{wang_aspect-based_2023,
	title = {Aspect-based {Sentiment} {Analysis} in {Document} -- {FOMC} {Meeting} {Minutes} on {Economic} {Projection}},
	url = {http://arxiv.org/abs/2108.04080},
	doi = {10.48550/arXiv.2108.04080},
	abstract = {The Federal Open Market Committee within the Federal Reserve System is responsible for managing inflation, maximizing employment, and stabilizing interest rates. Meeting minutes play an important role for market movements because they provide the birds eye view of how this economic complexity is constantly re-weighed. Therefore, There has been growing interest in analyzing and extracting sentiments on various aspects from large financial texts for economic projection. However, Aspect-based Sentiment Analysis is not widely used on financial data due to the lack of large labeled dataset. In this paper, I propose a model to train ABSA on financial documents under weak supervision and analyze its predictive power on various macroeconomic indicators.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Wang, Yifei},
	month = apr,
	year = {2023},
	note = {arXiv:2108.04080 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{tobback_between_2017,
	title = {Between {Hawks} and {Doves}: {Measuring} {Central} {Bank} {Communication}},
	issn = {1556-5068},
	shorttitle = {Between {Hawks} and {Doves}},
	url = {https://www.ssrn.com/abstract=2997481},
	doi = {10.2139/ssrn.2997481},
	language = {en},
	urldate = {2024-08-22},
	journal = {SSRN Electronic Journal},
	author = {Tobback, Ellen and Nardelli, Stefano and Martens, David},
	year = {2017},
}

@inproceedings{mathur_monopoly_2022,
	address = {Lisboa Portugal},
	title = {{MONOPOLY}: {Financial} {Prediction} from {MONetary} {POLicY} {Conference} {Videos} {Using} {Multimodal} {Cues}},
	isbn = {9781450392037},
	shorttitle = {{MONOPOLY}},
	url = {https://dl.acm.org/doi/10.1145/3503161.3548380},
	doi = {10.1145/3503161.3548380},
	language = {en},
	urldate = {2024-08-22},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Mathur, Puneet and Neerkaje, Atula and Chhibber, Malika and Sawhney, Ramit and Guo, Fuming and Dernoncourt, Franck and Dutta, Sanghamitra and Manocha, Dinesh},
	month = oct,
	year = {2022},
	pages = {2276--2285},
}

@inproceedings{matsui_using_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Using {Word} {Embedding} to {Reveal} {Monetary} {Policy} {Explanation} {Changes}},
	url = {https://aclanthology.org/2021.econlp-1.8},
	doi = {10.18653/v1/2021.econlp-1.8},
	abstract = {Documents have been an essential tool of communication for governments to announce their policy operations. Most policy announcements have taken the form of text to inform their new policies or changes to the public. To understand such policymakers' communication, many researchers exploit published policy documents. However, the methods well-used in other research domains such as sentiment analysis or topic modeling are not suitable for studying policy communications. Their training corpora and methods are not for policy documents where technical terminologies are used, and sentiment expressions are refrained. We leverage word embedding techniques to extract semantic changes in the monetary policy documents. Our empirical study shows that the policymaker uses different semantics according to the type of documents when they change their policy.},
	urldate = {2024-08-22},
	booktitle = {Proceedings of the {Third} {Workshop} on {Economics} and {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Matsui, Akira and Ren, Xiang and Ferrara, Emilio},
	editor = {Hahn, Udo and Hoste, Veronique and Stent, Amanda},
	month = nov,
	year = {2021},
	pages = {56--61},
}

@inproceedings{frunza_information_2020,
	address = {Barcelona, Spain (Online)},
	title = {Information {Extraction} from {Federal} {Open} {Market} {Committee} {Statements}},
	url = {https://aclanthology.org/2020.fnp-1.32},
	abstract = {We present a novel approach to unsupervised information extraction by identifying and extracting relevant concept-value pairs from textual data. The system's building blocks are domain agnostic, making it universally applicable. In this paper, we describe each component of the system and how it extracts relevant economic information from U.S. Federal Open Market Committee (FOMC) statements. Our methodology achieves an impressive 96\% accuracy for identifying relevant information for a set of seven economic indicators: household spending, inflation, unemployment, economic activity, fixed in-vestment, federal funds rate, and labor market.},
	urldate = {2024-08-22},
	booktitle = {Proceedings of the 1st {Joint} {Workshop} on {Financial} {Narrative} {Processing} and {MultiLing} {Financial} {Summarisation}},
	publisher = {COLING},
	author = {Frunza, Oana and Stanley, Morgan},
	editor = {El-Haj, Dr Mahmoud and Athanasakou, Dr Vasiliki and Ferradans, Dr Sira and Salzedo, Dr Catherine and Elhag, Dr Ans and Bouamor, Dr Houda and Litvak, Dr Marina and Rayson, Dr Paul and Giannakopoulos, Dr George and Pittaras, Nikiforos},
	month = dec,
	year = {2020},
	pages = {195--203},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2024-08-21},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{tetlock_more_2008,
	title = {More {Than} {Words}: {Quantifying} {Language} to {Measure} {Firms}' {Fundamentals}},
	volume = {63},
	issn = {0022-1082, 1540-6261},
	shorttitle = {More {Than} {Words}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2008.01362.x},
	doi = {10.1111/j.1540-6261.2008.01362.x},
	abstract = {ABSTRACT 
            We examine whether a simple quantitative measure of language can be used to predict individual firms' accounting earnings and stock returns. Our three main findings are: (1) the fraction of negative words in firm‐specific news stories forecasts low firm earnings; (2) firms' stock prices briefly underreact to the information embedded in negative words; and (3) the earnings and return predictability from negative words is largest for the stories that focus on fundamentals. Together these findings suggest that linguistic media content captures otherwise hard‐to‐quantify aspects of firms' fundamentals, which investors quickly incorporate into stock prices.},
	language = {en},
	number = {3},
	urldate = {2024-08-21},
	journal = {The Journal of Finance},
	author = {Tetlock, Paul C. and Saar‐Tsechansky, Maytal and Macskassy, Sofus},
	month = jun,
	year = {2008},
	pages = {1437--1467},
}

@inproceedings{delice_economic_2024,
	address = {Guatemala City, Guatemala},
	title = {The {Economic} {Value} of {Words}: an {Evaluation} of {News} for {Economic} {Analysis}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350361292},
	shorttitle = {The {Economic} {Value} of {Words}},
	url = {https://ieeexplore.ieee.org/document/10555884/},
	doi = {10.1109/LAEDC61552.2024.10555884},
	urldate = {2024-08-21},
	booktitle = {2024 {IEEE} {Latin} {American} {Electron} {Devices} {Conference} ({LAEDC})},
	publisher = {IEEE},
	author = {Delice, Pierre A. and Pinto, David and García-Guerrero, Victor M. and Hernández-López, Sergio},
	month = may,
	year = {2024},
	pages = {1--4},
}

@article{liu_online_2016,
	title = {Online {ARIMA} {Algorithms} for {Time} {Series} {Prediction}},
	volume = {30},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10257},
	doi = {10.1609/aaai.v30i1.10257},
	abstract = {Autoregressive integrated moving average (ARIMA) is one of the most popular linear models for time series forecasting due to its nice statistical properties and great flexibility. However, its parameters are estimated in a batch manner and its noise terms are often assumed to be strictly bounded, which restricts its applications and makes it inefficient for handling large-scale real data. In this paper, we propose online learning algorithms for estimating ARIMA models under relaxed assumptions on the noise terms, which is suitable to a wider range of applications and enjoys high computational efficiency. The idea of our ARIMA method is to reformulate the ARIMA model into a task of full information online optimization (without random noise terms). As a consequence, we can online estimation of the parameters in an efficient and scalable way. Furthermore, we analyze regret bounds of the proposed algorithms, which guarantee that our online ARIMA model is provably as good as the best ARIMA model in hindsight. Finally, our encouraging experimental results further validate the effectiveness and robustness of our method.},
	number = {1},
	urldate = {2024-08-21},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Chenghao and Hoi, Steven C.H. and Zhao, Peilin and Sun, Jianling},
	month = feb,
	year = {2016},
}

@article{bi_stock_2022,
	title = {Stock {Market} {Prediction} {Based} on {Financial} {News} {Text} {Mining} and {Investor} {Sentiment} {Recognition}},
	volume = {2022},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1563-5147, 1024-123X},
	url = {https://www.hindawi.com/journals/mpe/2022/2427389/},
	doi = {10.1155/2022/2427389},
	abstract = {The stock market is usually regarded as the bellwether of the economy, which can reflect the economic operation of a country or region. As a significant part of the financial market, the equity market plays a critical role in the financial sector. Whether in academia or investment field, stock market forecasts always excite great interest. Financial news is an important source of information in the financial market, which reflects the mood swings of investors and often goes hand in hand with the market trend. However, due to the unstructured and professional characteristics of financial news, there are challenges in accurately quantifying their emotional tendencies. This research is based on Hidden Markov Model (HMM) to segment financial news text. The recognition and classification of news emotion is carried out by bidirectional long short-term memory (BI-LSTM) algorithm, and long short-term memory(LSTM) model is trained with text emotion index and stock market transaction data to realize the prediction of stock market. The results show that BI-LSTM algorithm performs better than the emotional dictionary algorithm in emotional recognition. And the emotional index of financial news text can enhance the accuracy of stock market prediction to a certain extent. Compared with using stock market technical index and news text vector only, the prediction accuracy can be improved by about 2\%.},
	language = {en},
	urldate = {2024-08-21},
	journal = {Mathematical Problems in Engineering},
	author = {Bi, Jianxin},
	editor = {Yang, Zaoli},
	month = oct,
	year = {2022},
	pages = {1--9},
}

@article{khadjeh_nassirtoussi_text_2014,
	title = {Text mining for market prediction: {A} systematic review},
	volume = {41},
	issn = {09574174},
	shorttitle = {Text mining for market prediction},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417414003455},
	doi = {10.1016/j.eswa.2014.06.009},
	language = {en},
	number = {16},
	urldate = {2024-08-21},
	journal = {Expert Systems with Applications},
	author = {Khadjeh Nassirtoussi, Arman and Aghabozorgi, Saeed and Ying Wah, Teh and Ngo, David Chek Ling},
	month = nov,
	year = {2014},
	pages = {7653--7670},
}

@article{cao_ai_2023,
	title = {{AI} in {Finance}: {Challenges}, {Techniques}, and {Opportunities}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {{AI} in {Finance}},
	url = {https://dl.acm.org/doi/10.1145/3502289},
	doi = {10.1145/3502289},
	abstract = {AI in finance refers to the applications of AI techniques in financial businesses. This area has attracted attention for decades, with both classic and modern AI techniques applied to increasingly broader areas of finance, economy, and society. In contrast to reviews on discussing the problems, aspects, and opportunities of finance benefited from specific or some new-generation AI and data science (AIDS) techniques or the progress of applying specific techniques to resolving certain financial problems, this review offers a comprehensive and dense landscape of the overwhelming challenges, techniques, and opportunities of AIDS research in finance over the past decades. The challenges of financial businesses and data are first outlined, followed by a comprehensive categorization and a dense overview of the decades of AIDS research in finance. We then structure and illustrate the data-driven analytics and learning of financial businesses and data. A comparison, criticism, and discussion of classic versus modern AIDS techniques for finance follows. Finally, the open issues and opportunities to address future AIDS-empowered finance and finance-motivated AIDS research are discussed.},
	language = {en},
	number = {3},
	urldate = {2024-08-21},
	journal = {ACM Computing Surveys},
	author = {Cao, Longbing},
	month = mar,
	year = {2023},
	pages = {1--38},
}

@article{xing_natural_2018,
	title = {Natural language based financial forecasting: a survey},
	volume = {50},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Natural language based financial forecasting},
	url = {http://link.springer.com/10.1007/s10462-017-9588-9},
	doi = {10.1007/s10462-017-9588-9},
	language = {en},
	number = {1},
	urldate = {2024-08-21},
	journal = {Artificial Intelligence Review},
	author = {Xing, Frank Z. and Cambria, Erik and Welsch, Roy E.},
	month = jun,
	year = {2018},
	pages = {49--73},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	url = {https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	urldate = {2024-08-21},
	author = {Radford, Alec and Narasimhan, Karthik and Salisman, Tim and Sutskever},
	year = {2018},
}

@misc{chen_survey_2024,
	title = {A {Survey} on {Large} {Language} {Models} for {Critical} {Societal} {Domains}: {Finance}, {Healthcare}, and {Law}},
	shorttitle = {A {Survey} on {Large} {Language} {Models} for {Critical} {Societal} {Domains}},
	url = {http://arxiv.org/abs/2405.01769},
	doi = {10.48550/arXiv.2405.01769},
	abstract = {In the fast-evolving domain of artificial intelligence, large language models (LLMs) such as GPT-3 and GPT-4 are revolutionizing the landscapes of finance, healthcare, and law: domains characterized by their reliance on professional expertise, challenging data acquisition, high-stakes, and stringent regulatory compliance. This survey offers a detailed exploration of the methodologies, applications, challenges, and forward-looking opportunities of LLMs within these high-stakes sectors. We highlight the instrumental role of LLMs in enhancing diagnostic and treatment methodologies in healthcare, innovating financial analytics, and refining legal interpretation and compliance strategies. Moreover, we critically examine the ethics for LLM applications in these fields, pointing out the existing ethical concerns and the need for transparent, fair, and robust AI systems that respect regulatory norms. By presenting a thorough review of current literature and practical applications, we showcase the transformative impact of LLMs, and outline the imperative for interdisciplinary cooperation, methodological advancements, and ethical vigilance. Through this lens, we aim to spark dialogue and inspire future research dedicated to maximizing the benefits of LLMs while mitigating their risks in these precision-dependent sectors. To facilitate future research on LLMs in these critical societal domains, we also initiate a reading list that tracks the latest advancements under this topic, which will be continually updated: {\textbackslash}url\{https://github.com/czyssrs/LLM\_X\_papers\}.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Chen, Zhiyu Zoey and Ma, Jing and Zhang, Xinlu and Hao, Nan and Yan, An and Nourbakhsh, Armineh and Yang, Xianjun and McAuley, Julian and Petzold, Linda and Wang, William Yang},
	month = may,
	year = {2024},
	note = {arXiv:2405.01769 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{cieslak_non-monetary_2019,
	title = {Non-monetary news in central bank communication},
	volume = {118},
	issn = {00221996},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002219961830268X},
	doi = {10.1016/j.jinteco.2019.01.012},
	language = {en},
	urldate = {2024-08-15},
	journal = {Journal of International Economics},
	author = {Cieslak, Anna and Schrimpf, Andreas},
	month = may,
	year = {2019},
	pages = {293--315},
}

@article{jarocinski_deconstructing_2020,
	title = {Deconstructing {Monetary} {Policy} {Surprises}—{The} {Role} of {Information} {Shocks}},
	volume = {12},
	issn = {1945-7707},
	url = {https://www.aeaweb.org/articles?id=10.1257/mac.20180090},
	doi = {10.1257/mac.20180090},
	abstract = {Central bank announcements simultaneously convey information about monetary policy and the central bank's assessment of the economic outlook. This paper disentangles these two components and studies their effect on the economy using a structural vector autoregression. It relies on the information inherent in high-frequency co-movement of interest rates and stock prices around policy announcements: a surprise policy tightening raises interest rates and reduces stock prices, while the complementary positive central bank information shock raises both. These two shocks have intuitive and very different effects on the economy. Ignoring the central bank information shocks biases the inference on monetary policy nonneutrality.},
	language = {en},
	number = {2},
	urldate = {2024-08-15},
	journal = {American Economic Journal: Macroeconomics},
	author = {Jarociński, Marek and Karadi, Peter},
	month = apr,
	year = {2020},
	keywords = {Belief, Communication, Event Studies, Information and Knowledge, Insider Trading, Learning, Search, Unawareness, Interest Rates: Determination, Term Structure, and Effects, Financial Markets and the Macroeconomy, Monetary Policy, Central Banks and Their Policies, Information and Market Efficiency},
	pages = {1--43},
}

@article{altavilla_how_2020,
	title = {How do financial markets react to monetary policy signals?},
	volume = {73},
	url = {https://econpapers.repec.org/article/ecbecbrbu/2020_3a0073.htm},
	abstract = {We map ECB policy communications onto yield curve changes and study the information flow on monetary policy decision dates. We find that different monetary policy measures exert effects on different segments of the interest rate term structure, with policy rate changes mostly influencing the short-end of the curve and quantitative easing measures acting more on the long-end. The impact of forward guidance policies, on the other hand, reaches its peak at intermediate maturities. A by-product of this work is the publicly available Euro Area Monetary Policy Event-Study Database (EA-MPD), containing intraday asset price changes. JEL Classification: E43, E52, E58, G01, G21},
	urldate = {2024-08-15},
	journal = {Research Bulletin},
	author = {Altavilla, Carlo and Gürkaynak, Refet S. and Motto, Roberto and Ragusa, Giuseppe},
	month = jul,
	year = {2020},
	keywords = {Monetary policy surprise, event-study, yield curve},
}

@article{gebauer_how_2024,
	title = {How central bank communication affects the economy},
	url = {https://www.ecb.europa.eu/press/blog/date/2024/html/ecb.blog240731~61a58b1e4a.en.html},
	abstract = {The European Central Bank (ECB) is the central bank of the European Union countries which have adopted the euro. Our main task is to maintain price stability in the euro area and so preserve the purchasing power of the single currency.},
	language = {en},
	urldate = {2024-08-15},
	author = {Gebauer, Stefan and McGregor, Thomas and Schumacher, Julian},
	month = jul,
	year = {2024},
}

@article{du_financial_2024,
	title = {Financial {Sentiment} {Analysis}: {Techniques} and {Applications}},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Financial {Sentiment} {Analysis}},
	url = {https://dl.acm.org/doi/10.1145/3649451},
	doi = {10.1145/3649451},
	abstract = {Financial Sentiment Analysis (FSA) is an important domain application of sentiment analysis that has gained increasing attention in the past decade. FSA research falls into two main streams. The 
              first stream 
              focuses on defining tasks and developing techniques for FSA, and its main objective is to improve the performances of various FSA tasks by advancing methods and using/curating human-annotated datasets. The 
              second stream 
              of research focuses on using financial sentiment, implicitly or explicitly, for downstream applications on financial markets, which has received more research efforts. The main objective is to discover appropriate market applications for existing techniques. More specifically, the application of FSA mainly includes hypothesis testing and predictive modeling in financial markets. This survey conducts a comprehensive review of FSA research in both the technique and application areas and proposes several frameworks to help understand the two areas’ interactive relationship. This article defines a clearer scope for FSA studies and conceptualizes the FSA-investor sentiment-market sentiment relationship. Major findings, challenges, and future research directions for both FSA techniques and applications have also been summarized and discussed.},
	language = {en},
	number = {9},
	urldate = {2024-08-15},
	journal = {ACM Computing Surveys},
	author = {Du, Kelvin and Xing, Frank and Mao, Rui and Cambria, Erik},
	month = oct,
	year = {2024},
	pages = {1--42},
}

@article{hansen_transparency_2018,
	title = {Transparency and {Deliberation} {Within} the {FOMC}: {A} {Computational} {Linguistics} {Approach}*},
	volume = {133},
	issn = {0033-5533, 1531-4650},
	shorttitle = {Transparency and {Deliberation} {Within} the {FOMC}},
	url = {https://academic.oup.com/qje/article/133/2/801/4582916},
	doi = {10.1093/qje/qjx045},
	language = {en},
	number = {2},
	urldate = {2024-08-14},
	journal = {The Quarterly Journal of Economics},
	author = {Hansen, Stephen and McMahon, Michael and Prat, Andrea},
	month = may,
	year = {2018},
	pages = {801--870},
}

@article{hansen_shocking_2016,
	title = {Shocking language: {Understanding} the macroeconomic effects of central bank communication},
	volume = {99},
	issn = {00221996},
	shorttitle = {Shocking language},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022199615001828},
	doi = {10.1016/j.jinteco.2015.12.008},
	language = {en},
	urldate = {2024-08-14},
	journal = {Journal of International Economics},
	author = {Hansen, Stephen and McMahon, Michael},
	month = mar,
	year = {2016},
	pages = {S114--S133},
}

@article{rozkrut_quest_2007,
	title = {Quest for central bank communication: {Does} it pay to be “talkative”?},
	volume = {23},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01762680},
	shorttitle = {Quest for central bank communication},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0176268006000917},
	doi = {10.1016/j.ejpoleco.2006.09.011},
	language = {en},
	number = {1},
	urldate = {2024-08-14},
	journal = {European Journal of Political Economy},
	author = {Rozkrut, Marek and Rybiński, Krzysztof and Sztaba, Lucyna and Szwaja, Radosław},
	month = mar,
	year = {2007},
	pages = {176--206},
}

@book{european_central_bank_between_2017,
	address = {LU},
	title = {Between hawks and doves: measuring central bank communication.},
	shorttitle = {Between hawks and doves},
	url = {https://data.europa.eu/doi/10.2866/977130},
	language = {eng},
	urldate = {2024-08-14},
	publisher = {Publications Office},
	author = {{European Central Bank.}},
	year = {2017},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {9780387310732},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}

@incollection{bhan_tigtec_2023,
	address = {Cham},
	title = {{TIGTEC}: {Token} {Importance} {Guided} {TExt} {Counterfactuals}},
	volume = {14171},
	isbn = {9783031434174 9783031434181},
	shorttitle = {{TIGTEC}},
	url = {https://link.springer.com/10.1007/978-3-031-43418-1_30},
	language = {en},
	urldate = {2024-08-11},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}: {Research} {Track}},
	publisher = {Springer Nature Switzerland},
	author = {Bhan, Milan and Vittaut, Jean-Noël and Chesneau, Nicolas and Lesot, Marie-Jeanne},
	editor = {Bonchi, Francesco and Baralis, Elena and Gomez Rodriguez, Manuel and Plant, Claudia and Koutra, Danai},
	year = {2023},
	doi = {10.1007/978-3-031-43418-1_30},
	pages = {496--512},
}

@inproceedings{von_werra_evaluate_2022,
	address = {Abu Dhabi, UAE},
	title = {Evaluate \& {Evaluation} on the {Hub}: {Better} {Best} {Practices} for {Data} and {Model} {Measurements}},
	shorttitle = {Evaluate \& {Evaluation} on the {Hub}},
	url = {https://aclanthology.org/2022.emnlp-demos.13},
	doi = {10.18653/v1/2022.emnlp-demos.13},
	language = {en},
	urldate = {2024-08-11},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Von Werra, Leandro and Tunstall, Lewis and Thakur, Abhishek and Luccioni, Sasha and Thrush, Tristan and Piktus, Aleksandra and Marty, Felix and Rajani, Nazneen and Mustar, Victor and Ngo, Helen},
	year = {2022},
	pages = {128--136},
}

@article{chen_evaluation_2008,
	title = {Evaluation {Metrics} {For} {Language} {Models}},
	copyright = {In Copyright},
	url = {https://kilthub.cmu.edu/articles/Evaluation_Metrics_For_Language_Models/6605324/1},
	doi = {10.1184/R1/6605324.V1},
	abstract = {The most widely-used evaluation metric for language models for speech recognition is the perplexity of test data. While perplexities can be calculated efficiently and without access to a speech recognizer, they often do not correlate well with speech recognition word-error rates. In this research, we attempt to find a measure that like perplexity is easily calculated but which better predicts speech recognition performance. We investigate two approaches; first, we attempt to extend perplexity by using similar measures that utilize information about language models that perplexity ignores. Second, we attempt to imitate the word-error calculation without using a speech recognizer by artificially generating speech recognition lattices. To test our new metrics, we have built over thirty varied language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance. However, we conclude that none of these measures predict word-error rate sufficiently accurately to be effective tools for language model evaluation in speech recognition.},
	urldate = {2024-08-11},
	author = {Chen, Stanley F and Beeferman, Douglas and Rosenfeld, Roni},
	year = {2008},
	keywords = {89999 Information and Computing Sciences not elsewhere classified, FOS: Computer and information sciences},
	pages = {81295 Bytes},
}

@inproceedings{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	url = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	urldate = {2024-08-11},
	author = {Radford, Alec and Wu, Jeff and Child, R. and Luan, D. and Amodei, Dario and Sutskever, I.},
	year = {2019},
}

@article{jelinek_perplexitymeasure_1977,
	title = {Perplexity—a measure of the difficulty of speech recognition tasks},
	volume = {62},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/62/S1/S63/642598/Perplexity-a-measure-of-the-difficulty-of-speech},
	doi = {10.1121/1.2016299},
	abstract = {Using counterexamples, we show that vocabulary size and static and dynamic branching factors are all inadequate as measures of speech recognition complexity of finite state grammars. Information theoretic arguments show that perplexity (the logarithm of which is the familiar entropy) is a more appropriate measure of equivalent choice. It too has certain weaknesses which we discuss. We show that perplexity can also be applied to languages having no obvious statistical description, since an entropy-maximizing probability assignment can be found for any finite-state grammar. Table I shows perplexity values for some well-known speech recognition tasks. 
            Perplexity Vocabulary Dynamic 
            Phone Word size branching factor 
            IBM-Lasers 2.14 21.11 1000 1000 
            IBM-Raleigh 1.69 7.74 250 7.32 
            CMU-AIX05 1.52 6.41 1011 35},
	language = {en},
	number = {S1},
	urldate = {2024-08-11},
	journal = {The Journal of the Acoustical Society of America},
	author = {Jelinek, F. and Mercer, R. L. and Bahl, L. R. and Baker, J. K.},
	month = dec,
	year = {1977},
	pages = {S63--S63},
}

@book{bird_natural_2009,
	title = {Natural language processing with {Python}: analyzing text with the natural language toolkit},
	publisher = {" O'Reilly Media, Inc."},
	author = {Bird, Steven and Klein, Ewan and Loper, Edward},
	year = {2009},
}

@misc{henderson_zhang-shasha_nodate,
	title = {Zhang-{Shasha}: {Tree} edit distance in {Python} — {Zhang}-{Shasha} v1.2.0},
	url = {https://zhang-shasha.readthedocs.io/},
	urldate = {2024-08-10},
	author = {Henderson, Tim},
}

@article{zhang_simple_1989,
	title = {Simple {Fast} {Algorithms} for the {Editing} {Distance} between {Trees} and {Related} {Problems}},
	volume = {18},
	issn = {0097-5397, 1095-7111},
	url = {http://epubs.siam.org/doi/10.1137/0218082},
	doi = {10.1137/0218082},
	language = {en},
	number = {6},
	urldate = {2024-08-10},
	journal = {SIAM Journal on Computing},
	author = {Zhang, Kaizhong and Shasha, Dennis},
	month = dec,
	year = {1989},
	pages = {1245--1262},
}

@misc{hjelmqvist_fast_2006,
	title = {Fast, memory efficient {Levenshtein} algorithm},
	url = {https://www.codeproject.com/Articles/13525/Fast-memory-efficient-Levenshtein-algorithm-2},
	abstract = {A version of the Levenshtein algorithm that uses 2*Min(StrLen1,StrLen2) bytes instead of StrLen1*StrLen2 bytes.},
	language = {en-US},
	urldate = {2024-08-10},
	journal = {CodeProject},
	author = {Hjelmqvist, Sten},
	month = mar,
	year = {2006},
}

@misc{haldar_levenshtein_2011,
	title = {Levenshtein {Distance} {Technique} in {Dictionary} {Lookup} {Methods}: {An} {Improved} {Approach}},
	shorttitle = {Levenshtein {Distance} {Technique} in {Dictionary} {Lookup} {Methods}},
	url = {http://arxiv.org/abs/1101.1232},
	doi = {10.48550/arXiv.1101.1232},
	abstract = {Dictionary lookup methods are popular in dealing with ambiguous letters which were not recognized by Optical Character Readers. However, a robust dictionary lookup method can be complex as apriori probability calculation or a large dictionary size increases the overhead and the cost of searching. In this context, Levenshtein distance is a simple metric which can be an effective string approximation tool. After observing the effectiveness of this method, an improvement has been made to this method by grouping some similar looking alphabets and reducing the weighted difference among members of the same group. The results showed marked improvement over the traditional Levenshtein distance technique.},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Haldar, Rishin and Mukhopadhyay, Debajyoti},
	month = jan,
	year = {2011},
	note = {arXiv:1101.1232 [cs, math]},
	keywords = {Computer Science - Information Theory},
}

@article{levenshtein_binary_1965,
	title = {Binary codes capable of correcting deletions, insertions, and reversals},
	url = {https://www.semanticscholar.org/paper/Binary-codes-capable-of-correcting-deletions%2C-and-Levenshtein/b2f8876482c97e804bb50a5e2433881ae31d0cdd},
	abstract = {Semantic Scholar extracted view of "Binary codes capable of correcting deletions, insertions, and reversals" by V. Levenshtein},
	urldate = {2024-08-10},
	journal = {Soviet physics. Doklady},
	author = {Levenshtein, V.},
	year = {1965},
}

@inproceedings{artelt_evaluating_2021,
	address = {Orlando, FL, USA},
	title = {Evaluating {Robustness} of {Counterfactual} {Explanations}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9781728190488},
	url = {https://ieeexplore.ieee.org/document/9660058/},
	doi = {10.1109/SSCI50451.2021.9660058},
	urldate = {2024-08-04},
	booktitle = {2021 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	publisher = {IEEE},
	author = {Artelt, Andre and Vaquet, Valerie and Velioglu, Riza and Hinder, Fabian and Brinkrolf, Johannes and Schilling, Malte and Hammer, Barbara},
	month = dec,
	year = {2021},
	pages = {01--09},
}

@article{kenny_generating_2021,
	title = {On {Generating} {Plausible} {Counterfactual} and {Semi}-{Factual} {Explanations} for {Deep} {Learning}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17377},
	doi = {10.1609/aaai.v35i13.17377},
	abstract = {There is a growing concern that the recent progress made in AI, especially regarding the predictive competence of deep learning models, will be undermined by a failure to properly explain their operation and outputs. In response to this disquiet, counterfactual explanations have become very popular in eXplainable AI (XAI) due to their asserted computational, psychological, and legal benefits. In contrast however, semi-factuals (which appear to be equally useful) have surprisingly received no attention. Most counterfactual methods address tabular rather than image data, partly because the non-discrete nature of images makes good counterfactuals difficult to define; indeed, generating plausible counterfactual images which lie on the data manifold is also problematic. This paper advances a novel method for generating plausible counterfactuals and semi-factuals for black-box CNN classifiers doing computer vision. The present method, called PlausIble Exceptionality-based Contrastive Explanations (PIECE), modifies all “exceptional” features in a test image to be “normal” from the perspective of the counterfactual class, to generate plausible counterfactual images. Two controlled experiments compare this method to others in the literature, showing that PIECE generates highly plausible counterfactuals (and the best semi-factuals) on several benchmark measures.},
	language = {en},
	number = {13},
	urldate = {2024-08-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Kenny, Eoin M. and Keane, Mark T.},
	month = may,
	year = {2021},
	keywords = {Ethics -- Bias, Fairness, Transparency \& Privacy},
	pages = {11575--11585},
}

@inproceedings{samangouei_explaingan_2018,
	address = {Cham},
	title = {{ExplainGAN}: {Model} {Explanation} via {Decision} {Boundary} {Crossing} {Transformations}},
	isbn = {9783030012496},
	shorttitle = {{ExplainGAN}},
	doi = {10.1007/978-3-030-01249-6_41},
	abstract = {We introduce a new method for interpreting computer vision models: visually perceptible, decision-boundary crossing transformations. Our goal is to answer a simple question: why did a model classify an image as being of class A instead of class B? Existing approaches to model interpretation, including saliency and explanation-by-nearest neighbor, fail to visually illustrate examples of transformations required for a specific input to alter a model’s prediction. On the other hand, algorithms for creating decision-boundary crossing transformations (e.g., adversarial examples) produce differences that are visually imperceptible and do not enable insightful explanation. To address this we introduce ExplainGAN, a generative model that produces visually perceptible decision-boundary crossing transformations. These transformations provide high-level conceptual insights which illustrate how a model makes decisions. We validate our model using both traditional quantitative interpretation metrics and introduce a new validation scheme for our approach and generative models more generally.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Samangouei, Pouya and Saeedi, Ardavan and Nakagawa, Liam and Silberman, Nathan},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Model interpretation, Neural networks},
	pages = {681--696},
}

@inproceedings{van_looveren_interpretable_2021,
	address = {Cham},
	title = {Interpretable {Counterfactual} {Explanations} {Guided} by {Prototypes}},
	isbn = {9783030865207},
	doi = {10.1007/978-3-030-86520-7_40},
	abstract = {We propose a fast, model agnostic method for finding interpretable counterfactual explanations of classifier predictions by using class prototypes. We show that class prototypes, obtained using either an encoder or through class specific k-d trees, significantly speed up the search for counterfactual instances and result in more interpretable explanations. We quantitatively evaluate interpretability of the generated counterfactuals to illustrate the effectiveness of our method on an image and tabular dataset, respectively MNIST and Breast Cancer Wisconsin (Diagnostic). Additionally, we propose a principled approach to handle categorical variables and illustrate our method on the Adult (Census) dataset. Our method also eliminates the computational bottleneck that arises because of numerical gradient evaluation for black box models.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}. {Research} {Track}},
	publisher = {Springer International Publishing},
	author = {Van Looveren, Arnaud and Klaise, Janis},
	editor = {Oliver, Nuria and Pérez-Cruz, Fernando and Kramer, Stefan and Read, Jesse and Lozano, Jose A.},
	year = {2021},
	pages = {650--665},
}

@article{altmeyer_faithful_2024,
	title = {Faithful {Model} {Explanations} through {Energy}-{Constrained} {Conformal} {Counterfactuals}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/28956},
	doi = {10.1609/aaai.v38i10.28956},
	abstract = {Counterfactual explanations offer an intuitive and straightforward way to explain black-box models and offer algorithmic recourse to individuals. To address the need for plausible explanations, existing work has primarily relied on surrogate models to learn how the input data is distributed. This effectively reallocates the task of learning realistic explanations for the data from the model itself to the surrogate. Consequently, the generated explanations may seem plausible to humans but need not necessarily describe the behaviour of the black-box model faithfully. We formalise this notion of faithfulness through the introduction of a tailored evaluation metric and propose a novel algorithmic framework for generating Energy-Constrained Conformal Counterfactuals that are only as plausible as the model permits. Through extensive empirical studies, we demonstrate that ECCCo reconciles the need for faithfulness and plausibility. In particular, we show that for models with gradient access, it is possible to achieve state-of-the-art performance without the need for surrogate models. To do so, our framework relies solely on properties defining the black-box model itself by leveraging recent advances in energy-based modelling and conformal prediction. To our knowledge, this is the first venture in this direction for generating faithful counterfactual explanations. Thus, we anticipate that ECCCo can serve as a baseline for future research. We believe that our work opens avenues for researchers and practitioners seeking tools to better distinguish trustworthy from unreliable models.},
	language = {en},
	number = {10},
	urldate = {2024-08-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Altmeyer, Patrick and Farmanbar, Mojtaba and Deursen, Arie van and Liem, Cynthia C. S.},
	month = mar,
	year = {2024},
	keywords = {ML: Deep Generative Models \& Autoencoders},
	pages = {10829--10837},
}

@inproceedings{shah_trillion_2023,
	address = {Toronto, Canada},
	title = {Trillion {Dollar} {Words}: {A} {New} {Financial} {Dataset}, {Task} \& {Market} {Analysis}},
	shorttitle = {Trillion {Dollar} {Words}},
	url = {https://aclanthology.org/2023.acl-long.368},
	doi = {10.18653/v1/2023.acl-long.368},
	abstract = {Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences financial markets. In this study, we develop a novel task of hawkish-dovish classification and benchmark various pre-trained language models on the proposed dataset. Using the best-performing model (RoBERTa-large), we construct a measure of monetary policy stance for the FOMC document release days. To evaluate the constructed measure, we study its impact on the treasury market, stock market, and macroeconomic indicators. Our dataset, models, and code are publicly available on Huggingface and GitHub under CC BY-NC 4.0 license.},
	urldate = {2024-06-29},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Shah, Agam and Paturi, Suvan and Chava, Sudheer},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {6664--6679},
}

@inproceedings{ma_corpus_2006,
	address = {Genoa, Italy},
	title = {Corpus {Support} for {Machine} {Translation} at {LDC}},
	url = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/754_pdf.pdf},
	abstract = {This paper describes LDC's efforts in collecting, creating and processing different types of linguistic data, including lexicons, parallel text, multiple translation corpora, and human assessment of translation quality, to support the research and development in Machine Translation. Through a combination of different procedures and core technologies, the LDC was able to create very large, high quality, and cost-efficient corpora, which have contributed significantly to recent advances in Machine Translation. Multiple translation corpora and human assessment together facilitate, validate and improve automatic evaluation metrics, which are vital to the development of MT systems. The Bilingual Internet Text Search (BITS) and Champollion sentence aligner enable the finding and processing of large quantities of parallel text. All specifications and tools used by LDC and described in the paper are or will be available to the general public.},
	urldate = {2024-05-15},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'06)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Ma, Xiaoyi and Cieri, Christopher},
	editor = {Calzolari, Nicoletta and Choukri, Khalid and Gangemi, Aldo and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Tapias, Daniel},
	month = may,
	year = {2006},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-05-06},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2024-04-30},
	journal = {CoRR},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-04-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{zhu_texygen_2018,
	address = {New York, NY, USA},
	series = {{SIGIR} '18},
	title = {Texygen: {A} {Benchmarking} {Platform} for {Text} {Generation} {Models}},
	isbn = {9781450356572},
	shorttitle = {Texygen},
	url = {https://doi.org/10.1145/3209978.3210080},
	doi = {10.1145/3209978.3210080},
	abstract = {We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and improve the reproductivity and reliability of future research work in text generation.},
	urldate = {2024-04-29},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
	month = jun,
	year = {2018},
	keywords = {benchmarking, evaluation metrics, text generation},
	pages = {1097--1100},
}

@article{hovy_principles_2002,
	title = {Principles of {Context}-{Based} {Machine} {Translation} {Evaluation}},
	volume = {17},
	issn = {1573-0573},
	url = {https://doi.org/10.1023/A:1025510524115},
	doi = {10.1023/A:1025510524115},
	abstract = {This article defines a Framework for Machine Translation Evaluation( FEMTI) which relates the quality model used to evaluate a machinetranslation system to the purpose and context of the system. Ourproposal attempts to put together, into a coherent picture, previousattempts to structure a domain characterised by overall complexity andlocal difficulties. In this article, we first summarise theseattempts, then present an overview of the ISO/IEC guidelines forsoftware evaluation (ISO/IEC 9126 and ISO/IEC 14598). As anapplication of these guidelines to machine translation software, weintroduce FEMTI, a framework that is made of two interrelatedclassifications or taxonomies. The first classification enablesevaluators to define an intended context of use, while the links tothe second classification generate a relevant quality model (qualitycharacteristics and metrics) for the respective context. The secondclassification provides definitions of various metrics used bythe community. Further on, as part of ongoing, long-term research, weexplain how metrics are analyzed, first from the general pointof view of “meta-evaluation”, then focusing on examples. Finally, weshow how consensus towards the present framework is sought for, andhow feedback from the community is taken into account in the FEMTIlife-cycle.},
	language = {en},
	number = {1},
	urldate = {2024-04-29},
	journal = {Machine Translation},
	author = {Hovy, Eduard and King, Margaret and Popescu-Belis, Andrei},
	month = mar,
	year = {2002},
	keywords = {MT evaluation, context-based evaluation, evaluation metrics, quality models},
	pages = {43--75},
}

@inproceedings{white_arpa_1994,
	address = {Columbia, Maryland, USA},
	title = {The {ARPA} {MT} {Evaluation} {Methodologies}: {Evolution}, {Lessons}, and {Future} {Approaches}},
	shorttitle = {The {ARPA} {MT} {Evaluation} {Methodologies}},
	url = {https://aclanthology.org/1994.amta-1.25},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the {First} {Conference} of the {Association} for {Machine} {Translation} in the {Americas}},
	author = {White, John S. and O'Connell, Theresa A. and O'Mara, Francis E.},
	month = oct,
	year = {1994},
}

@inproceedings{papineni_bleu_2002,
	address = {USA},
	series = {{ACL} '02},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	shorttitle = {{BLEU}},
	url = {https://dl.acm.org/doi/10.3115/1073083.1073135},
	doi = {10.3115/1073083.1073135},
	abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	month = jul,
	year = {2002},
	pages = {311--318},
}

@inproceedings{wu_polyjuice_2021,
	address = {Online},
	title = {Polyjuice: {Generating} {Counterfactuals} for {Explaining}, {Evaluating}, and {Improving} {Models}},
	shorttitle = {Polyjuice},
	url = {https://aclanthology.org/2021.acl-long.523},
	doi = {10.18653/v1/2021.acl-long.523},
	abstract = {While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70\% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {6707--6723},
}

@article{gilo_general_2024,
	title = {A {General} {Search}-{Based} {Framework} for {Generating} {Textual} {Counterfactual} {Explanations}},
	volume = {38},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29764},
	doi = {10.1609/aaai.v38i16.29764},
	abstract = {One of the prominent methods for explaining the decision of a machine-learning classifier is by a counterfactual example. 
Most current algorithms for generating such examples in the textual domain are based on generative language models.  Generative models, however, are trained to minimize a specific loss function in order to fulfill certain requirements for the generated texts.  Any change in the requirements may necessitate costly retraining, thus potentially limiting their applicability. 
In this paper, we present a general search-based framework for generating counterfactual explanations in the textual domain.   
Our framework is model-agnostic, domain-agnostic, anytime, and does not require retraining in order to adapt to changes in the user requirements.  
We model the task as a search problem in a space where the initial state is the classified text, and the goal state is a text in a given target class.  Our framework includes domain-independent modification operators, but can also exploit domain-specific knowledge through specialized operators. The search algorithm attempts to find a text from the target class with minimal user-specified distance from the original classified object.},
	number = {16},
	urldate = {2024-04-27},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Gilo, Daniel and Markovitch, Shaul},
	month = mar,
	year = {2024},
	pages = {18073--18081},
}

@article{dathathri_plug_2019,
	title = {Plug and {Play} {Language} {Models}: {A} {Simple} {Approach} to {Controlled} {Text} {Generation}},
	shorttitle = {Plug and {Play} {Language} {Models}},
	url = {https://www.semanticscholar.org/paper/Plug-and-Play-Language-Models%3A-A-Simple-Approach-to-Dathathri-Madotto/e04a80263d252a3d8a382ba37a249b9345620570},
	abstract = {Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.},
	urldate = {2024-04-27},
	journal = {ArXiv},
	author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, J. and Liu, Rosanne},
	month = sep,
	year = {2019},
}

@inproceedings{fern_text_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Text {Counterfactuals} via {Latent} {Optimization} and {Shapley}-{Guided} {Search}},
	url = {https://aclanthology.org/2021.emnlp-main.452},
	doi = {10.18653/v1/2021.emnlp-main.452},
	abstract = {We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model's prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent white-box and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals.},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Fern, Xiaoli and Pope, Quintin},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {5578--5593},
}

@inproceedings{wu_mask_2019,
	address = {Macao, China},
	title = {Mask and {Infill}: {Applying} {Masked} {Language} {Model} for {Sentiment} {Transfer}},
	isbn = {9780999241141},
	shorttitle = {Mask and {Infill}},
	url = {https://www.ijcai.org/proceedings/2019/732},
	doi = {10.24963/ijcai.2019/732},
	abstract = {This paper focuses on the task of sentiment transfer on non-parallel text, which modifies sentiment attributes (e.g., positive or negative) of sentences while preserving their attribute-independent contents. Existing methods adopt RNN encoder-decoder structure to generate a new sentence of a target sentiment word by word, which is trained on a particular dataset from scratch and have limited ability to produce satisfactory sentences. When people convert the sentiment attribute of a given sentence, a simple but effective approach is to only replace the sentiment tokens of the sentence with other expressions indicative of the target sentiment, instead of building a new sentence from scratch. Such a process is very similar to the task of Text Infilling or Cloze. With this intuition, we propose a two steps approach: Mask and Infill. In the {\textbackslash}emph\{mask\} step, we identify and mask the sentiment tokens of a given sentence. In the {\textbackslash}emph\{infill\} step, we utilize a pre-trained Masked Language Model (MLM) to infill the masked positions by predicting words or phrases conditioned on the context{\textbackslash}footnote\{In this paper, {\textbackslash}emph\{content\} and {\textbackslash}emph\{context\} are equivalent, {\textbackslash}emph\{style\}, {\textbackslash}emph\{attribute\} and {\textbackslash}emph\{label\} are equivalent.\}and target sentiment. We evaluate our model on two review datasets {\textbackslash}emph\{Yelp\} and {\textbackslash}emph\{Amazon\} by quantitative, qualitative, and human evaluations. Experimental results demonstrate that our model achieve state-of-the-art performance on both accuracy and BLEU scores.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Wu, Xing and Zhang, Tao and Zang, Liangjun and Han, Jizhong and Hu, Songlin},
	month = aug,
	year = {2019},
	pages = {5271--5277},
}

@inproceedings{robeer_generating_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Generating {Realistic} {Natural} {Language} {Counterfactuals}},
	url = {https://aclanthology.org/2021.findings-emnlp.306},
	doi = {10.18653/v1/2021.findings-emnlp.306},
	abstract = {Counterfactuals are a valuable means for understanding decisions made by ML systems. However, the counterfactuals generated by the methods currently available for natural language text are either unrealistic or introduce imperceptible changes. We propose CounterfactualGAN: a method that combines a conditional GAN and the embeddings of a pretrained BERT encoder to model-agnostically generate realistic natural language text counterfactuals for explaining regression and classification tasks. Experimental results show that our method produces perceptibly distinguishable counterfactuals, while outperforming four baseline methods on fidelity and human judgments of naturalness, across multiple datasets and multiple predictive models.},
	urldate = {2024-04-27},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Robeer, Marcel and Bex, Floris and Feelders, Ad},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {3611--3625},
}

@inproceedings{yang_generating_2020,
	address = {Barcelona, Spain (Online)},
	title = {Generating {Plausible} {Counterfactual} {Explanations} for {Deep} {Transformers} in {Financial} {Text} {Classification}},
	url = {https://aclanthology.org/2020.coling-main.541},
	doi = {10.18653/v1/2020.coling-main.541},
	abstract = {Corporate mergers and acquisitions (M\&A) account for billions of dollars of investment globally every year and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust/accurate model, but be able to generate useful explanations to garner a user's trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user's trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Yang, Linyi and Kenny, Eoin and Ng, Tin Lok James and Yang, Yi and Smyth, Barry and Dong, Ruihai},
	editor = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
	month = dec,
	year = {2020},
	pages = {6150--6160},
}

@misc{gat_faithful_2023,
	title = {Faithful {Explanations} of {Black}-box {NLP} {Models} {Using} {LLM}-generated {Counterfactuals}},
	url = {http://arxiv.org/abs/2310.00603},
	doi = {10.48550/arXiv.2310.00603},
	abstract = {Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Gat, Yair and Calderon, Nitay and Feder, Amir and Chapanin, Alexander and Sharma, Amit and Reichart, Roi},
	month = nov,
	year = {2023},
	note = {arXiv:2310.00603 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{yang_exploring_2021,
	address = {Online},
	title = {Exploring the {Efficacy} of {Automatically} {Generated} {Counterfactuals} for {Sentiment} {Analysis}},
	url = {https://aclanthology.org/2021.acl-long.26},
	doi = {10.18653/v1/2021.acl-long.26},
	abstract = {While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation. A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data.},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Linyi and Li, Jiazheng and Cunningham, Padraig and Zhang, Yue and Smyth, Barry and Dong, Ruihai},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {306--316},
}

@inproceedings{bhan_enhancing_2023,
	address = {Toronto, Canada},
	title = {Enhancing textual counterfactual explanation intelligibility through {Counterfactual} {Feature} {Importance}},
	url = {https://aclanthology.org/2023.trustnlp-1.19},
	doi = {10.18653/v1/2023.trustnlp-1.19},
	abstract = {Textual counterfactual examples explain a prediction by modifying the tokens of an initial instance in order to flip the outcome of a classifier. Even under sparsity constraint, counterfactual generation can lead to numerous changes from the initial text, making the explanation hard to understand. We propose Counterfactual Feature Importance, a method to make non-sparse counterfactual explanations more intelligible. Counterfactual Feature Importance assesses token change importance between an instance to explain and its counterfactual example. We develop two ways of computing Counterfactual Feature Importance, respectively based on classifier gradient computation and counterfactual generator loss evolution during counterfactual search. Then we design a global version of Counterfactual Feature Importance, providing rich information about semantic fields globally impacting classifier predictions. Counterfactual Feature Importance enables to focus on impacting parts of counterfactual explanations, making counterfactual explanations involving numerous changes more understandable.},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 3rd {Workshop} on {Trustworthy} {Natural} {Language} {Processing} ({TrustNLP} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Bhan, Milan and Vittaut, Jean-noel and Chesneau, Nicolas and Lesot, Marie-jeanne},
	editor = {Ovalle, Anaelia and Chang, Kai-Wei and Mehrabi, Ninareh and Pruksachatkun, Yada and Galystan, Aram and Dhamala, Jwala and Verma, Apurv and Cao, Trista and Kumar, Anoop and Gupta, Rahul},
	month = jul,
	year = {2023},
	pages = {221--231},
}

@inproceedings{treviso_crest_2023,
	address = {Toronto, Canada},
	title = {{CREST}: {A} {Joint} {Framework} for {Rationalization} and {Counterfactual} {Text} {Generation}},
	shorttitle = {{CREST}},
	url = {https://aclanthology.org/2023.acl-long.842},
	doi = {10.18653/v1/2023.acl-long.842},
	abstract = {Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing CREST (ContRastive Edits with Sparse raTionalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework leads to improvements in counterfactual quality, model robustness, and interpretability. First, CREST generates valid counterfactuals that are more natural than those produced by previous methods, and subsequently can be used for data augmentation at scale, reducing the need for human-generated examples. Second, we introduce a new loss function that leverages CREST counterfactuals to regularize selective rationales and show that this regularization improves both model robustness and rationale quality, compared to methods that do not leverage CREST counterfactuals. Our results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model's predictions.},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Treviso, Marcos and Ross, Alexis and Guerreiro, Nuno M. and Martins, André},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {15109--15126},
}

@inproceedings{dixit_core_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{CORE}: {A} {Retrieve}-then-{Edit} {Framework} for {Counterfactual} {Data} {Generation}},
	shorttitle = {{CORE}},
	url = {https://aclanthology.org/2022.findings-emnlp.216},
	doi = {10.18653/v1/2022.findings-emnlp.216},
	abstract = {Counterfactual data augmentation (CDA) – i.e., adding minimally perturbed inputs during training – helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of perturbations, limiting their effectiveness. We present Counterfactual Generation via Retrieval and Editing (CORE), a retrieval-augmented generation framework for creating diverse counterfactual perturbations for CDA. For each training example, CORE first performs a dense retrieval over a task-related unlabeled text corpus using a learned bi-encoder and extracts relevant counterfactual excerpts. CORE then incorporates these into prompts to a large language model with few-shot learning capabilities, for counterfactual editing. Conditioning language model edits on naturally occurring data results in more diverse perturbations. Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches. We also show that the CORE retrieval framework can be used to encourage diversity in manually authored perturbations.},
	urldate = {2024-04-27},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Dixit, Tanay and Paranjape, Bhargavi and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {2964--2984},
}

@inproceedings{krause_gedi_2021,
	address = {Punta Cana, Dominican Republic},
	title = {{GeDi}: {Generative} {Discriminator} {Guided} {Sequence} {Generation}},
	shorttitle = {{GeDi}},
	url = {https://aclanthology.org/2021.findings-emnlp.424},
	doi = {10.18653/v1/2021.findings-emnlp.424},
	urldate = {2024-04-27},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {4929--4952},
}

@inproceedings{madaan_counterfactual_2023,
	address = {Raleigh, NC, USA},
	title = {Counterfactual {Sentence} {Generation} with {Plug}-and-{Play} {Perturbation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9781665462990},
	url = {https://ieeexplore.ieee.org/document/10136170/},
	doi = {10.1109/SaTML54575.2023.00028},
	urldate = {2024-04-27},
	booktitle = {2023 {IEEE} {Conference} on {Secure} and {Trustworthy} {Machine} {Learning} ({SaTML})},
	publisher = {IEEE},
	author = {Madaan, Nishtha and Saha, Diptikalyan and Bedathur, Srikanta},
	month = feb,
	year = {2023},
	pages = {306--315},
}

@inproceedings{betti_relevance-based_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {Relevance-based {Infilling} for {Natural} {Language} {Counterfactuals}},
	isbn = {9798400701245},
	url = {https://doi.org/10.1145/3583780.3615029},
	doi = {10.1145/3583780.3615029},
	abstract = {Counterfactual explanations are a natural way for humans to gain understanding and trust in the outcomes of complex machine learning algorithms. In the context of natural language processing, generating counterfactuals is particularly challenging as it requires the generated text to be fluent, grammatically correct, and meaningful. In this study, we improve the current state of the art for the generation of such counterfactual explanations for text classifiers. Our approach, named RELITC (Relevance-based Infilling for Textual Counterfactuals), builds on the idea of masking a fraction of text tokens based on their importance in a given prediction task and employs a novel strategy, based on the entropy of their associated probability distributions, to determine the infilling order of these tokens. Our method uses less time than competing methods to generate counterfactuals that require less changes, are closer to the original text and preserve its content better, while being competitive in terms of fluency. We demonstrate the effectiveness of the method on four different datasets and show the quality of its outcomes in a comparison with human generated counterfactuals.},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Betti, Lorenzo and Abrate, Carlo and Bonchi, Francesco and Kaltenbrunner, Andreas},
	month = oct,
	year = {2023},
	keywords = {NLP, counterfactuals, explainability, masked language model},
	pages = {88--98},
}

@inproceedings{ross_explaining_2021,
	address = {Online},
	title = {Explaining {NLP} {Models} via {Minimal} {Contrastive} {Editing} ({MiCE})},
	url = {https://aclanthology.org/2021.findings-acl.336},
	doi = {10.18653/v1/2021.findings-acl.336},
	urldate = {2024-04-27},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Ross, Alexis and Marasović, Ana and Peters, Matthew},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {3840--3852},
}

@inproceedings{madaan_generate_2021,
	title = {Generate {Your} {Counterfactuals}: {Towards} {Controlled} {Counterfactual} {Generation} for {Text}},
	volume = {35},
	shorttitle = {Generate {Your} {Counterfactuals}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17594},
	doi = {10.1609/aaai.v35i15.17594},
	abstract = {Machine Learning has seen tremendous growth recently, which has led to a larger adaptation of ML systems for educational assessments, credit risk, healthcare, employment, criminal justice, to name a few. The trustworthiness of ML and NLP systems is a crucial aspect and requires a guarantee that the decisions they make are fair and robust. Aligned with this, we propose a novel framework GYC, to generate a set of exhaustive counterfactual text, which are crucial for testing these ML systems. Our main contributions include a) We introduce GYC, a framework to generate counterfactual samples such that the generation is plausible, diverse, goal-oriented, and effective, b) We generate counterfactual samples, that can direct the generation towards a corresponding {\textbackslash}texttt\{condition\} such as named-entity tag, semantic role label, or sentiment. Our experimental results on various domains show that GYC generates counterfactual text samples exhibiting the above four properties. GYC generates counterfactuals that can act as test cases to evaluate a model and any text debiasing algorithm.},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Madaan, Nishtha and Padhi, Inkit and Panwar, Naveen and Saha, Diptikalyan},
	month = may,
	year = {2021},
	pages = {13516--13524},
}

@inproceedings{fonseca_setting_2023,
	address = {New York, NY, USA},
	series = {{EAAMO} '23},
	title = {Setting the {Right} {Expectations}: {Algorithmic} {Recourse} {Over} {Time}},
	isbn = {9798400703812},
	shorttitle = {Setting the {Right} {Expectations}},
	url = {https://dl.acm.org/doi/10.1145/3617694.3623251},
	doi = {10.1145/3617694.3623251},
	abstract = {Algorithmic systems are often called upon to assist in high-stakes decision making. In light of this, algorithmic recourse, the principle wherein individuals should be able to take action against an undesirable outcome made by an algorithmic system, is receiving growing attention. The bulk of the literature on algorithmic recourse to-date focuses primarily on how to provide recourse to a single individual, overlooking a critical element: the effects of a continuously changing context. Disregarding these effects on recourse is a significant oversight, since, in almost all cases, recourse consists of an individual making a first, unfavorable attempt, and then being given an opportunity to make one or several attempts at a later date — when the context might have changed. This can create false expectations, as initial recourse recommendations may become less reliable over time due to model drift and competition for access to the favorable outcome between individuals. In this work we propose an agent-based simulation framework for studying the effects of a continuously changing environment on algorithmic recourse. In particular, we identify two main effects that can alter the reliability of recourse for individuals represented by the agents: (1) competition with other agents acting upon recourse, and (2) competition with new agents entering the environment. Our findings highlight that only a small set of specific parameterizations result in algorithmic recourse that is reliable for agents over time. Consequently, we argue that substantial additional work is needed to understand recourse reliability over time, and to develop recourse methods that reward agents’ effort.},
	urldate = {2024-04-24},
	booktitle = {Proceedings of the 3rd {ACM} {Conference} on {Equity} and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {Association for Computing Machinery},
	author = {Fonseca, João and Bell, Andrew and Abrate, Carlo and Bonchi, Francesco and Stoyanovich, Julia},
	month = oct,
	year = {2023},
	pages = {1--11},
}

@misc{wachter_counterfactual_2018,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	shorttitle = {Counterfactual {Explanations} without {Opening} the {Black} {Box}},
	url = {http://arxiv.org/abs/1711.00399},
	doi = {10.48550/arXiv.1711.00399},
	abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	urldate = {2024-04-23},
	publisher = {arXiv},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	month = mar,
	year = {2018},
	note = {arXiv:1711.00399 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}
