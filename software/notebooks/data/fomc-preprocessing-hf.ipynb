{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0dc345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfFolder, login\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48254530",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".hf_token\", 'r') as token:\n",
    "    login(token.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f095e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"gtfintechlab/fomc_communication\"\n",
    "repository_id = \"TextCEsInFinance/fomc_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98def3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(dataset_id, encoding='utf-8')\n",
    "\n",
    "# Training and testing datasets\n",
    "train_dataset = dataset['train'].to_pandas()\n",
    "test_dataset = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d315042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sentence', 'year', 'label']\n",
    "\n",
    "train_dataset = train_dataset[columns]\n",
    "test_dataset = test_dataset[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd30279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need this to directly output the class names when using the pipeline without mapping the labels later.\n",
    "# Extract the number of classes and their names\n",
    "\n",
    "# LABEL_2: Neutral\n",
    "# LABEL_1: Hawkish\n",
    "# LABEL_0: Dovish\n",
    "\n",
    "num_labels = 3\n",
    "class_names = ['dovish', 'hawkish', 'neutral'] # hardcoded because not included in repo\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "id2label = {i: label for i, label in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66629926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign column for text label\n",
    "train_dataset = train_dataset.assign(text_label=train_dataset['label'].apply(lambda x: id2label[x]))\n",
    "test_dataset = test_dataset.assign(text_label=test_dataset['label'].apply(lambda x: id2label[x]))\n",
    "\n",
    "# Rename text column\n",
    "train_dataset = train_dataset.rename(columns={\"sentence\": \"text\"})\n",
    "test_dataset = test_dataset.rename(columns={\"sentence\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.drop_duplicates(subset=['text'], keep='last')\n",
    "test_dataset = test_dataset.drop_duplicates(subset=['text'], keep='last')\n",
    "\n",
    "# Check for duplicate sentences in the dataset\n",
    "duplicates = pd.merge(train_dataset, test_dataset, how='inner', on=['text'])['text']\n",
    "\n",
    "# Drop duplicate sentences\n",
    "train_dataset = train_dataset[~train_dataset['text'].isin(duplicates)]\n",
    "\n",
    "# Check if no more duplicates exist\n",
    "assert pd.merge(train_dataset, test_dataset, how='inner', on=['text'])['text'].empty\n",
    "assert len(train_dataset) == len(pd.unique(train_dataset['text']))\n",
    "assert len(test_dataset) == len(pd.unique(test_dataset['text']))\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8028596",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_csv('./data/fomc/fomc_train_preprocessed.csv', index_label='index')\n",
    "test_dataset.to_csv('./data/fomc/fomc_test_preprocessed.csv', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9d17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign targets for counterfactual generation\n",
    "\n",
    "test_hawkish = test_dataset[test_dataset['text_label'] == 'hawkish']\n",
    "test_dovish = test_dataset[test_dataset['text_label'] == 'dovish']\n",
    "test_neutral = test_dataset[test_dataset['text_label'] == 'neutral']\n",
    "\n",
    "split = int(len(test_neutral) / 2)\n",
    "\n",
    "# Half of the neutral sentences will be transformed into dovish, rest into hawkish \n",
    "\n",
    "test_neutral_to_hawkish = test_neutral.iloc[split:]\n",
    "test_neutral_to_dovish = test_neutral.iloc[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93f2282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels\n",
    "\n",
    "test_hawkish = test_hawkish.assign(target=2)\n",
    "test_dovish = test_dovish.assign(target=2)\n",
    "\n",
    "test_neutral_to_hawkish = test_neutral_to_hawkish.assign(target=1)\n",
    "test_neutral_to_dovish = test_neutral_to_dovish.assign(target=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e6bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets = pd.concat([test_hawkish, test_dovish, test_neutral_to_hawkish, test_neutral_to_dovish])\n",
    "test_targets = test_targets.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "test_targets.to_csv('./data/fomc/test_with_targets.csv', index_label='index', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f95121",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(i, (i + 1)%3, (i+2)%3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('TextCEsInFinance/fomc-communication-counterfactual', encoding='utf-8')\n",
    "\n",
    "# Training and testing datasets\n",
    "test_dataset = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708792e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.iloc[340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b1064f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hawkish = test_dataset[test_dataset['text_label'] == 'hawkish']\n",
    "test_dovish = test_dataset[test_dataset['text_label'] == 'dovish']\n",
    "test_neutral = test_dataset[test_dataset['text_label'] == 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_hawkish), len(test_dovish), len(test_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9585222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(test_hawkish)/2)\n",
    "test_hawkish_0 = test_hawkish.iloc[split:].assign(target=0)\n",
    "test_hawkish_2 = test_hawkish.iloc[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd07f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(test_dovish)/2)\n",
    "test_dovish_1 = test_dovish.iloc[split:].assign(target=1)\n",
    "test_dovish_2 = test_dovish.iloc[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803be4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([test_neutral, test_hawkish_0, test_hawkish_2, test_dovish_1, test_dovish_2]).sort_values('index')\n",
    "dovish_1 = full[(full['label'] == 0) & (full['target'] == 1)]\n",
    "dovish_2 = full[(full['label'] == 0) & (full['target'] == 2)]\n",
    "hawkish_0 = full[(full['label'] == 1) & (full['target'] == 0)]\n",
    "hawkish_2 = full[(full['label'] == 1) & (full['target'] == 2)]\n",
    "neutral_0 = full[(full['label'] == 2) & (full['target'] == 0)]\n",
    "neutral_1 = full[(full['label'] == 2) & (full['target'] == 1)]\n",
    "\n",
    "print(len(dovish_1), len(dovish_2), len(hawkish_0), len(hawkish_2), len(neutral_0), len(neutral_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c33c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "full.to_csv('full.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506a0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
